{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying code from scratch\n",
    "This file implements this tutorial (https://huggingface.co/docs/transformers/tasks/sequence_classification) with the PHP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "csv_name is the result of a NIST dataset run through Kyle's parser<br>\n",
    "folder_name is where all the data files are actually stored>br>\n",
    "num_cases is the number of test cases you want to process (concretely, the first num_cases data points are analyzed)\n",
    "\n",
    "file_formatting incrementally removes all types of comments by specific regex. The data files seem to have HTML, Python, and PHP style comments. Then we remove newlines.\n",
    "\n",
    "files is a list of strings containing the formatted file contents<br>\n",
    "labels is a list of 0s and 1s, where 0 = good and 1 - bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# php csv\n",
    "csv_name = 'parsed_data.csv'\n",
    "folder_name = 'data/2022-05-12-php-test-suite-sqli-v1-0-0/'\n",
    "\n",
    "df = pd.read_csv(csv_name)\n",
    "df_len = df.shape[0]\n",
    "# num_cases = int(df_len / 4)\n",
    "num_cases = 100\n",
    "df = df.head(num_cases) # take top (num_cases) files for now\n",
    "filenames = df['file_location']\n",
    "\n",
    "def file_formatting(file_location):\n",
    "    file_path = file_location\n",
    "    raw_contents = open(folder_name + file_path, \"r\").read()\n",
    "    remove = re.sub(\"(<!--.*?-->)\", \"\", raw_contents, flags=re.DOTALL) # html\n",
    "    remove = re.sub('#.*?\\n', '', remove, flags=re.DOTALL) # python\n",
    "    remove = re.sub('\\/\\*\\*[^*]*\\*+([^/][^*]*\\*+)*\\/', '', remove, flags=re.S) # php\n",
    "    remove = remove.replace('\\n', '').replace(' ','') # newlines\n",
    "    return remove\n",
    "\n",
    "# data contains strings of all files\n",
    "files = []\n",
    "for f in filenames:\n",
    "    try:\n",
    "        fstring = file_formatting(f)\n",
    "    except:\n",
    "        pass\n",
    "    files.append(fstring)\n",
    "\n",
    "# get labels\n",
    "labels = df['state']\n",
    "def replace_good_bad(lst):\n",
    "    mapping = {\"good\": 0, \"bad\": 1}\n",
    "    return [mapping.get(item, item) for item in lst]\n",
    "labels = replace_good_bad(labels)\n",
    "\n",
    "# create df of files and labels\n",
    "data = pd.DataFrame({'label': labels, 'file': files})\n",
    "#print(data, files, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset\n",
    "Converts dataframe into Tensorflow dataset, adds a train/test split to dataset, converts to correct Dataset input type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c5f6b7fae54ad4b301a58d0f3fb6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4a59259a2c45d5b25536f40116d81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'microsoft/codebert-base' # code\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# creating tf dataset\n",
    "tf_data = Dataset.from_pandas(data)\n",
    "tf_data = tf_data.train_test_split(test_size=0.3)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"file\"], truncation=True)\n",
    "#tf_data = tf_data.map(preprocess_function, batched=True)\n",
    "\n",
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"file\"], truncation=True)\n",
    "\n",
    "tf_data = tf_data.map(tokenize_dataset)\n",
    "#tf_data = model.prepare_tf_dataset(tf_data, batch_size=16, shuffle=True, tokenizer=tokenizer)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding evaluation\n",
    "This cell is separate because the evaluate imports can cause errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter and label initialization\n",
    "Play with the hyperparams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "# positive = vulnerable\n",
    "\n",
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tf_data[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset to correct input type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tf_data[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tf_data[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model!\n",
    "This will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4/4 [==============================] - 124s 31s/step - loss: 0.6495 - val_loss: 0.4978 - accuracy: 0.8333\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 97s 25s/step - loss: 0.4469 - val_loss: 0.4581 - accuracy: 0.8333\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 99s 26s/step - loss: 0.4112 - val_loss: 0.4700 - accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef8fff14f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "\n",
    "callbacks = [metric_callback]\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test one case\n",
    "Ensure labels and outputs are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# one case\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "inputs = tokenizer(files[0], return_tensors=\"tf\")\n",
    "\n",
    "#model = TFAutoModelForSequenceClassification.from_pretrained(model)\n",
    "logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "print(model.config.id2label[predicted_class_id])\n",
    "print(labels[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy\n",
    "This cell computes the accuracy of the model a bit slowly. Feel free to improve and add model evaluation methods are desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "NEGATIVE 0\n",
      "NEGATIVE 1\n",
      "NEGATIVE 1\n",
      "NEGATIVE 0\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "\n",
    "for idx in range(len(files)):\n",
    "    inputs = tokenizer(files[idx], return_tensors=\"tf\")\n",
    "    logits = model(**inputs).logits\n",
    "    predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "    print(idx, model.config.id2label[predicted_class_id], labels[idx])\n",
    "    if predicted_class_id == labels[idx]:\n",
    "        acc = acc + 1\n",
    "\n",
    "print(acc/len(files))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qmind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "346b0f21ce2bbe12bf6ec23d8ea2c439ad378e82ba816a1507dc93d905551b8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
