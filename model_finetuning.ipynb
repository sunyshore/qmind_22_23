{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Classifiying code by fine-tuning a pre-trained model\n","This model (https://huggingface.co/mrm8488/codebert-base-finetuned-detect-insecure-code), given a piece of code, returns 0 for secure code and 1 for insecure code. We fine-tune the model to see if any improvements are made."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# test imports\n","import re\n","import numpy as np\n","import pandas as pd\n","from datasets import Dataset\n","import torch\n","\n","from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, TrainingArguments, Trainer, AutoModelForSequenceClassification\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data preprocessing\n","csv_name is the result of a NIST dataset run through Kyle's parser<br>\n","folder_name is where all the data files are actually stored>br>\n","num_cases is the number of test cases you want to process (concretely, the first num_cases data points are analyzed)\n","\n","file_formatting incrementally removes all types of comments by specific regex. The data files seem to have HTML, Python, and PHP style comments. Then we remove newlines.\n","\n","files is a list of strings containing the formatted file contents<br>\n","labels is a list of 0s and 1s, where 0 = good and 1 - bad"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# php csv\n","csv_name = 'parsed_data.csv'\n","folder_name = 'data/2022-05-12-php-test-suite-sqli-v1-0-0/'\n","\n","df = pd.read_csv(csv_name)\n","df_len = df.shape[0]\n","# num_cases = int(df_len / 4)\n","num_cases = 100\n","df = df.head(num_cases) # take top (num_cases) files for now\n","filenames = df['file_location']\n","\n","def file_formatting(file_location):\n","    file_path = file_location\n","    raw_contents = open(folder_name + file_path, \"r\").read()\n","    remove = re.sub(\"(<!--.*?-->)\", \"\", raw_contents, flags=re.DOTALL) # html\n","    remove = re.sub('#.*?\\n', '', remove, flags=re.DOTALL) # python\n","    remove = re.sub('\\/\\*\\*[^*]*\\*+([^/][^*]*\\*+)*\\/', '', remove, flags=re.S) # php\n","    remove = remove.replace('\\n', '').replace(' ','') # newlines\n","    return remove\n","\n","# data contains strings of all files\n","files = []\n","for f in filenames:\n","    try:\n","        fstring = file_formatting(f)\n","    except:\n","        pass\n","    files.append(fstring)\n","\n","# get labels\n","state = df['state']\n","def replace_good_bad(lst):\n","    mapping = {\"good\": 0, \"bad\": 1}\n","    return [mapping.get(item, item) for item in lst]\n","state = replace_good_bad(state)\n","\n","# create df of files and labels\n","data = pd.DataFrame({'label': state, 'file': files})\n","#print(data, files, labels)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Model selection and setup\n","You can uncomment the model_name to choose which model's tokenizer you want to use"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-03-02 19:40:26.890837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["#model_name = 'bert-base-cased' # natural language\n","model_name = 'microsoft/codebert-base' # code\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# do not run unless you are just testing\n","This cell is for testing purposes to ensure all imports and dependencies above work. This cell does not run fast on large datasets. To fine-tune using a large dataset, run the cell below"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"]},{"name":"stdout","output_type":"stream","text":["<class 'str'> <class 'list'>\n","4/4 [==============================] - 150s 31s/step - loss: 0.5383\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fbc9565c3d0>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# tester model for smaller datasets\n","\n","#data = np.array(data)\n","labels = np.asarray(state)\n","print(type(files[0]), type(files))\n","tokenized_data = tokenizer(files, return_tensors=\"np\", padding=True)\n","# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n","tokenized_data = dict(tokenized_data)\n","\n","model.compile(optimizer=Adam(3e-5))\n","\n","labels = np.array(data[\"label\"])\n","model.fit(tokenized_data, labels)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# run this\n","This cell creates a Tensorflow dataset to better handle large inputs. Then it fits the model"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"438a12251db04fc4a9ea8d9c4b0d9a2f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"]},{"name":"stdout","output_type":"stream","text":["6/6 [==============================] - 316s 51s/step - loss: 0.4933\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f798e3a8550>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# tensorflow model for larger datasets\n","\n","# creating tf dataset\n","tf_data = Dataset.from_pandas(data)\n","\n","def tokenize_dataset(data):\n","    # Keys of the returned dictionary will be added to the dataset as columns\n","    return tokenizer(data[\"file\"], padding=\"max_length\", truncation=True)\n","\n","tf_data = tf_data.map(tokenize_dataset, batched=True)\n","tf_data = model.prepare_tf_dataset(tf_data, batch_size=16, shuffle=True, tokenizer=tokenizer)\n","\n","# Load and compile our model\n","# Lower learning rates are often better for fine-tuning transformers\n","model.compile(optimizer=Adam(3e-5))\n","model.fit(tf_data)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### One case tester cell\n","To run many test cases at once, run the next cell"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0 tf.Tensor([2.0462415], shape=(1,), dtype=float32) 0\n"]},{"data":{"text/plain":["'LABEL_0'"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["inputs = tokenizer(files[0], return_tensors=\"tf\", truncation=True, padding='max_length')\n","pt_labels = torch.tensor([1]).unsqueeze(0).numpy()  # Batch size 1\n","outputs = model(**inputs, labels=pt_labels)\n","loss = outputs.loss\n","logits = outputs.logits\n","print(np.argmax(logits.numpy()), loss, state[0])\n","\n","logits = model(**inputs).logits\n","predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","model.config.id2label[predicted_class_id]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Model accuracy\n","This cell computes the accuracy of the model a bit slowly. Feel free to improve and add model evaluation methods are desired"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0 0 tf.Tensor([2.0462415], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.04916], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.1357431], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.1071458], shape=(1,), dtype=float32) 1\n","0 0 tf.Tensor([2.099639], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.0569355], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.167201], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.0916145], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.1226583], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.1774137], shape=(1,), dtype=float32) 1\n","0 0 tf.Tensor([2.1934183], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.1057508], shape=(1,), dtype=float32) 1\n","0 0 tf.Tensor([2.0568833], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.0796993], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.1138654], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.1853392], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.128327], shape=(1,), dtype=float32) 0\n","0 0 tf.Tensor([2.101945], shape=(1,), dtype=float32) 0\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m      9\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[0;32m---> 11\u001b[0m logits \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     12\u001b[0m predicted_class_id \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39margmax(logits, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[1;32m     13\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mid2label[predicted_class_id]\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/engine/training.py:557\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    555\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/engine/base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1096\u001b[0m ):\n\u001b[0;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:433\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    432\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 433\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:1360\u001b[0m, in \u001b[0;36mTFRobertaForSequenceClassification.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39m@unpack_inputs\u001b[39m\n\u001b[1;32m   1332\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(ROBERTA_INPUTS_DOCSTRING\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mbatch_size, sequence_length\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1333\u001b[0m \u001b[39m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     training: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1353\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001b[39m.\u001b[39mTensor]]:\n\u001b[1;32m   1354\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m \u001b[39m    labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m \u001b[39m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m \u001b[39m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m \u001b[39m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1361\u001b[0m         input_ids,\n\u001b[1;32m   1362\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1363\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1364\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1365\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1366\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1367\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1368\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1369\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1370\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1372\u001b[0m     sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1373\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output, training\u001b[39m=\u001b[39mtraining)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/engine/base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1096\u001b[0m ):\n\u001b[0;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:433\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    432\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 433\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:745\u001b[0m, in \u001b[0;36mTFRobertaMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    743\u001b[0m     head_mask \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers\n\u001b[0;32m--> 745\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    746\u001b[0m     hidden_states\u001b[39m=\u001b[39;49membedding_output,\n\u001b[1;32m    747\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    748\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    749\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    750\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    751\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    752\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    753\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    754\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    755\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    756\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    757\u001b[0m )\n\u001b[1;32m    759\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    760\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(hidden_states\u001b[39m=\u001b[39msequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/engine/base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1096\u001b[0m ):\n\u001b[0;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:542\u001b[0m, in \u001b[0;36mTFRobertaEncoder.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    538\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[1;32m    540\u001b[0m past_key_value \u001b[39m=\u001b[39m past_key_values[i] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    543\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    544\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    545\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    546\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    547\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    548\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    549\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    550\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    552\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    554\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/engine/base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1096\u001b[0m ):\n\u001b[0;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:451\u001b[0m, in \u001b[0;36mTFRobertaLayer.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[1;32m    439\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    440\u001b[0m     hidden_states: tf\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[tf\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    449\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    452\u001b[0m         input_tensor\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    453\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    454\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    455\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    456\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    457\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    458\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    459\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    463\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/engine/base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1096\u001b[0m ):\n\u001b[0;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:364\u001b[0m, in \u001b[0;36mTFRobertaAttention.call\u001b[0;34m(self, input_tensor, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[1;32m    354\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    355\u001b[0m     input_tensor: tf\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m     training: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    363\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[tf\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 364\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(\n\u001b[1;32m    365\u001b[0m         hidden_states\u001b[39m=\u001b[39;49minput_tensor,\n\u001b[1;32m    366\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    367\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    368\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    369\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    370\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    371\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    372\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    373\u001b[0m     )\n\u001b[1;32m    374\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_output(\n\u001b[1;32m    375\u001b[0m         hidden_states\u001b[39m=\u001b[39mself_outputs[\u001b[39m0\u001b[39m], input_tensor\u001b[39m=\u001b[39minput_tensor, training\u001b[39m=\u001b[39mtraining\n\u001b[1;32m    376\u001b[0m     )\n\u001b[1;32m    377\u001b[0m     \u001b[39m# add attentions (possibly with past_key_value) if we output them\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/engine/base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1096\u001b[0m ):\n\u001b[0;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:311\u001b[0m, in \u001b[0;36mTFRobertaSelfAttention.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     attention_probs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmultiply(attention_probs, head_mask)\n\u001b[0;32m--> 311\u001b[0m attention_output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mmatmul(attention_probs, value_layer)\n\u001b[1;32m    312\u001b[0m attention_output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(attention_output, perm\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m])\n\u001b[1;32m    314\u001b[0m \u001b[39m# (batch_size, seq_len_q, all_head_size)\u001b[39;00m\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:3667\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3664\u001b[0m     \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39mbatch_mat_mul_v3(\n\u001b[1;32m   3665\u001b[0m         a, b, adj_x\u001b[39m=\u001b[39madjoint_a, adj_y\u001b[39m=\u001b[39madjoint_b, Tout\u001b[39m=\u001b[39moutput_type, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m   3666\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3667\u001b[0m     \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mbatch_mat_mul_v2(\n\u001b[1;32m   3668\u001b[0m         a, b, adj_x\u001b[39m=\u001b[39;49madjoint_a, adj_y\u001b[39m=\u001b[39;49madjoint_b, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   3670\u001b[0m \u001b[39m# Neither matmul nor sparse_matmul support adjoint, so we conjugate\u001b[39;00m\n\u001b[1;32m   3671\u001b[0m \u001b[39m# the matrix and use transpose instead. Conj() is a noop for real\u001b[39;00m\n\u001b[1;32m   3672\u001b[0m \u001b[39m# matrices.\u001b[39;00m\n\u001b[1;32m   3673\u001b[0m \u001b[39mif\u001b[39;00m adjoint_a:\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:1571\u001b[0m, in \u001b[0;36mbatch_mat_mul_v2\u001b[0;34m(x, y, adj_x, adj_y, name)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   1570\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1571\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   1572\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mBatchMatMulV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, y, \u001b[39m\"\u001b[39;49m\u001b[39madj_x\u001b[39;49m\u001b[39m\"\u001b[39;49m, adj_x, \u001b[39m\"\u001b[39;49m\u001b[39madj_y\u001b[39;49m\u001b[39m\"\u001b[39;49m, adj_y)\n\u001b[1;32m   1573\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   1574\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["acc = 0\n","\n","for idx in range(num_cases):\n","    inputs = tokenizer(files[idx], return_tensors=\"tf\", truncation=True, padding='max_length')\n","    pt_labels = torch.tensor([1]).unsqueeze(0).numpy()  # Batch size 1\n","    #print(state.shape)\n","    outputs = model(**inputs, labels=pt_labels)\n","    loss = outputs.loss\n","    logits = outputs.logits\n","\n","    logits = model(**inputs).logits\n","    predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n","    model.config.id2label[predicted_class_id]\n","\n","    print(np.argmax(logits.numpy()), predicted_class_id, loss, state[idx])\n","\n","    if (np.argmax(logits.numpy()) == state[idx]):\n","        acc = acc + 1\n","\n","print(acc / num_cases)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# buggy code: do not run!\n","This cell implements the 'Train with PyTorch Trainer' section of this (https://huggingface.co/docs/transformers/training)\n","\n","It does not work on my computer :sad:"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /Users/mercy/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"microsoft/codebert-base\",\n","  \"architectures\": [\n","    \"RobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file tf_model.h5 from cache at /Users/mercy/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/tf_model.h5\n","All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Requirement already satisfied: sklearn in /Users/mercy/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages (0.0.post1)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d335d934b4449ab8903ace519c8815d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"AttributeError","evalue":"'TFRobertaForSequenceClassification' object has no attribute 'to'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 35\u001b[0m\n\u001b[1;32m     30\u001b[0m tf_data \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mprepare_tf_dataset(tf_data, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m     32\u001b[0m \u001b[39m#small_train_dataset = tf_data[\"train\"].shuffle(seed=42).select(range(50))\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m#small_eval_dataset = tf_data[\"test\"].shuffle(seed=42).select(range(50))\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     36\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     37\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[1;32m     38\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtf_data,\n\u001b[1;32m     39\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mtf_data,\n\u001b[1;32m     40\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/transformers/trainer.py:445\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer\n\u001b[1;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_model_on_device:\n\u001b[0;32m--> 445\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_move_model_to_device(model, args\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    447\u001b[0m \u001b[39m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_model_parallel:\n","File \u001b[0;32m~/opt/anaconda3/envs/qmind2/lib/python3.9/site-packages/transformers/trainer.py:688\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_move_model_to_device\u001b[39m(\u001b[39mself\u001b[39m, model, device):\n\u001b[0;32m--> 688\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mparallel_mode \u001b[39m==\u001b[39m ParallelMode\u001b[39m.\u001b[39mTPU \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mtie_weights\u001b[39m\u001b[39m\"\u001b[39m):\n","\u001b[0;31mAttributeError\u001b[0m: 'TFRobertaForSequenceClassification' object has no attribute 'to'"]}],"source":["# pytorch init\n","pt_model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","training_args = TrainingArguments(output_dir=\"test_trainer\")\n","\n","%pip install sklearn\n","import evaluate\n","\n","metric = evaluate.load(\"accuracy\")\n","\n","# pytorch training\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n","\n","\n","# creating tf dataset\n","tf_data = Dataset.from_pandas(data)\n","#tf_data = tf_data.train_test_split(test_size=0.3)\n","#print(tf_data)\n","\n","def tokenize_dataset(data):\n","    # Keys of the returned dictionary will be added to the dataset as columns\n","    return tokenizer(data[\"file\"], padding=\"max_length\", truncation=True)\n","\n","tf_data = tf_data.map(tokenize_dataset, batched=True)\n","tf_data = model.prepare_tf_dataset(tf_data, batch_size=16, shuffle=True, tokenizer=tokenizer)\n","\n","#small_train_dataset = tf_data[\"train\"].shuffle(seed=42).select(range(50))\n","#small_eval_dataset = tf_data[\"test\"].shuffle(seed=42).select(range(50))\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tf_data,\n","    eval_dataset=tf_data,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"]}],"metadata":{"kernelspec":{"display_name":"qmind","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3d8a7bff6ed677d019fab7f8cc5db9ab315bc494189335bb20c92090bb8b7e50"}}},"nbformat":4,"nbformat_minor":2}
