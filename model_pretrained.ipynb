{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying code with a pretrained model (CodeBERT)\n",
    "This model (https://huggingface.co/mrm8488/codebert-base-finetuned-detect-insecure-code), given a piece of code, returns 0 for secure code and 1 for insecure code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Anaconda3\\envs\\qmind\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# test imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "csv_name is the result of a NIST dataset run through Kyle's parser<br>\n",
    "folder_name is where all the data files are actually stored>br>\n",
    "num_cases is the number of test cases you want to process (concretely, the first num_cases data points are analyzed)\n",
    "\n",
    "file_formatting incrementally removes all types of comments by specific regex. The data files seem to have HTML, Python, and PHP style comments. Then we remove newlines.\n",
    "\n",
    "files is a list of strings containing the formatted file contents<br>\n",
    "labels is a list of 0s and 1s, where 0 = good and 1 - bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# php csv\n",
    "csv_name = 'parsed_data.csv'\n",
    "folder_name = 'data/2022-05-12-php-test-suite-sqli-v1-0-0/'\n",
    "\n",
    "df = pd.read_csv(csv_name)\n",
    "df_len = df.shape[0]\n",
    "# num_cases = int(df_len / 4)\n",
    "num_cases = 100\n",
    "df = df.head(num_cases) # take top (num_cases) files for now\n",
    "filenames = df['file_location']\n",
    "\n",
    "def file_formatting(file_location):\n",
    "    file_path = file_location\n",
    "    raw_contents = open(folder_name + file_path, \"r\").read()\n",
    "    remove = re.sub(\"(<!--.*?-->)\", \"\", raw_contents, flags=re.DOTALL) # html\n",
    "    remove = re.sub('#.*?\\n', '', remove, flags=re.DOTALL) # python\n",
    "    remove = re.sub('\\/\\*\\*[^*]*\\*+([^/][^*]*\\*+)*\\/', '', remove, flags=re.S) # php\n",
    "    remove = remove.replace('\\n', '').replace(' ','') # newlines\n",
    "    return remove\n",
    "\n",
    "# data contains strings of all files\n",
    "files = []\n",
    "for f in filenames:\n",
    "    try:\n",
    "        fstring = file_formatting(f)\n",
    "    except:\n",
    "        pass\n",
    "    files.append(fstring)\n",
    "\n",
    "# get labels\n",
    "state = df['state']\n",
    "def replace_good_bad(lst):\n",
    "    mapping = {\"good\": 0, \"bad\": 1}\n",
    "    return [mapping.get(item, item) for item in lst]\n",
    "state = replace_good_bad(state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup\n",
    "You need to run this before running the cell below it!\n",
    "\n",
    "This initializes the model and prints a test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mrm8488/codebert-base-finetuned-detect-insecure-code')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('mrm8488/codebert-base-finetuned-detect-insecure-code')\n",
    "\n",
    "# single use test\n",
    "inputs = tokenizer(files[0], return_tensors=\"pt\", truncation=True, padding='max_length')\n",
    "pt_labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=pt_labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(np.argmax(logits.detach().numpy()), state[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model running\n",
    "Super scuffed but this interates through num_cases test cases and returns the accuracy of the model. However, I think the pretrained model is very predisposed to only returning 1. I also don't understand what the pt_labels means :/\n",
    "\n",
    "But hey we have a baseline accuracy of ~0.15 for a Transformers model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(0.0174, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0137, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0329, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0291, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0729, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.1860, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0064, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0822, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0390, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0245, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0094, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0279, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0272, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0594, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0067, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0041, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0139, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0198, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0291, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.2048, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0165, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.1231, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0592, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0058, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0111, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0890, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0237, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0093, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0385, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0428, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0184, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0076, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0132, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0055, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0568, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0033, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0185, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0936, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0382, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0186, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0282, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0069, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0221, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0598, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0748, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0152, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0390, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0281, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0244, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0075, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0656, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0344, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0447, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0291, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0126, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0401, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0688, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0559, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0079, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0332, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.4905, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.2799, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0471, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0200, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.1164, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0259, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0119, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0251, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0222, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0756, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0143, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0655, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0546, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0971, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0159, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0898, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0545, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0321, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0582, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0132, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.3120, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0150, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.2138, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.6096, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0127, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0185, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0323, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0309, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0454, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0208, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0173, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0121, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0986, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0127, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0654, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.2012, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0715, grad_fn=<NllLossBackward0>) 0\n",
      "1 tensor(0.0621, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.1032, grad_fn=<NllLossBackward0>) 1\n",
      "1 tensor(0.0250, grad_fn=<NllLossBackward0>) 0\n",
      "0.15\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "\n",
    "for idx in range(num_cases):\n",
    "    inputs = tokenizer(files[idx], return_tensors=\"pt\", truncation=True, padding='max_length')\n",
    "    pt_labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    #print(state.shape)\n",
    "    outputs = model(**inputs, labels=pt_labels)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    print(np.argmax(logits.detach().numpy()), loss, state[idx])\n",
    "    if (np.argmax(logits.detach().numpy()) == state[idx]):\n",
    "        acc = acc + 1\n",
    "\n",
    "print(acc / num_cases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qmind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "346b0f21ce2bbe12bf6ec23d8ea2c439ad378e82ba816a1507dc93d905551b8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
